from collections import Counter

import pandas as pd
import numpy as np

import nltk
from nltk.tokenize import sent_tokenize, word_tokenize
import catboost
from pymystem3 import Mystem
import pymorphy2

from typing import Any

import warnings
warnings.filterwarnings('ignore')


class WordNetReviewGenerator:
    """
        –ì–ª–æ–±–∞–ª—å–Ω—ã–π –∫–ª–∞—Å—Å –¥–ª—è –Ω–∞—Ö–æ–∂–¥–µ–Ω–∏—è –ø–æ–ª–µ–∑–Ω—ã—Ö –±–∏–≥—Ä–∞–º–º –≤ –æ—Ç–∑—ã–≤–∞—Ö
        –∏ —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏—è –Ω–∞ –∏—Ö –æ—Å–Ω–æ–≤–µ –∏—Ç–æ–≥–æ–≤–æ–≥–æ –æ—Ç—á–µ—Ç–∞ 
    """
    
    def __init__(self, clf:catboost.CatBoostClassifier, extractor:Any, emb_model:Any, context:Any, user:Any, message_id:int):
        """
            –û–±—Ä–∞–±–æ—Ç–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ —ç–∫–∑–µ–º–ø–ª—è—Ä–∞ –∫–ª–∞—Å—Å–∞
            @clf - –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ–≥–æ –±—É—Å—Ç–∏–Ω–≥–∞
            @raw_data - '—Å—ã—Ä—ã–µ' –¥–∞–Ω–Ω—ã–µ
            @extractor - RUSentimentExtractor –º–æ–¥–µ–ª—å –∏–ª–∏ –ª—é–±–∞—è –¥—Ä—É–≥–∞—è –º–æ–¥–µ–ª—å
            @emb_model -  –º–æ–¥–µ–ª—å –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Ä—É—Å—Å–∫–æ–≥–æ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞
            @context - –∫–æ–Ω—Ç–µ–∫—Å—Ç —Ç–µ–ª–µ–≥—Ä–∞–º, –≥–¥–µ –ª–µ–∂–∏—Ç –±–æ—Ç, –∫—É–¥–∞ –±—É–¥–µ–º —Å–ª–∞—Ç—å —Å–æ–æ–±—â–µ–Ω–∏—è –æ –ø—Ä–æ—Ü–µ—Å—Å–µ
            @user - –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å, –¥–ª—è –∫–æ—Ç–æ—Ä–æ–≥–æ —Ä–∞–±–æ—Ç–∞–µ—Ç wordnet
            @message_id - id —Å–æ–æ–±—â–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä–æ–µ –±—É–¥–µ—Ç —Ä–µ–¥–∞—á–∏—Ç—å—Å—è –ø–æ –º–µ—Ä–µ –∞–Ω–∞–ª–∏–∑–∞
        """
        
        self.clf = clf
        self.extractor = extractor
        self.russian_stopwords = nltk.corpus.stopwords.words("russian")
        self.lemmer = Mystem()
        self.morph = pymorphy2.MorphAnalyzer()
        self.emb_model = emb_model

        self.context = context
        self.user = user
        self.message_id = message_id
        
    def run(self, raw_data:pd.DataFrame):
        """
            –§—É–Ω–∫—Ü–∏—è –∑–∞–ø—É—Å–∫–∞ –∞–ª–≥–æ—Ä–∏—Ç–º–∞
        """
        self.raw_data = raw_data
        self.data_prep()
        
        self.context.bot.edit_message_text(
                    chat_id=self.user.external_id,
                    message_id=self.message_id,
                    text='ü™ì –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∞–ª —Å–æ–±—Ä–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ.'
                )
        
        self.bigrams_work()

        self.context.bot.edit_message_text(
                    chat_id=self.user.external_id,
                    message_id=self.message_id,
                    text='‚ö±Ô∏è –í—ã–¥–µ–ª–∏–ª –ø–æ–ª–µ–∑–Ω—ã–µ —Ç–æ–ø–∏–∫–∏.'
                )
        
        self.classification()
        
        self.context.bot.edit_message_text(
                    chat_id=self.user.external_id,
                    message_id=self.message_id,
                    text='üßΩ –ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–ª –∑–Ω–∞—á–∏–º—ã–µ —Ç–æ–ø–∏–∫–∏.'
                )

        return self.output()
        
    def output(self) -> None:
        def prepare_category(t:str, keywords:Any, good_cls:Any):
            """
                –§—É–Ω–∫—Ü–∏—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –º–∞—Å—Å–∏–≤–∞ –¥–∞–Ω–Ω—ã—Ö –ø–æ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏
                'pos' - –ø–æ–∑–∏—Ç–∏–≤–Ω–∞—è, 'neg' - –Ω–µ–≥–∞—Ç–∏–≤–Ω–∞—è
                
                @t - —Ç–∏–ø —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏
                @keywords - —Ç–µ –±–∏–≥—Ä–∞–º–º—ã, –∫–æ—Ç–æ—Ä—ã–µ –ø—Ä–æ—à–ª–∏ —Ñ–∏–ª—å—Ç—Ä –∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—é
                @good_cls - –ø–ª–∞—Å—Ç –æ—Ç–∑—ã–≤–æ–≤
            """
            
            end_data = {}
            garbage = {}

            if self.raw_data.shape[0] <= 100:
                lower_por = 0
                upper_por = 5
            elif t == 'pos':
                if self.raw_data.shape[0] > 100 and self.raw_data.shape[0] <= 1500:
                    lower_por = 2
                    upper_por = 7
                elif self.raw_data.shape[0] > 1500:
                    lower_por = 3
                    upper_por = 10
            else:
                if self.raw_data.shape[0] > 100 and self.raw_data.shape[0] <= 1500:
                    lower_por = 0
                    upper_por = 100
                elif self.raw_data.shape[0] > 1500:
                    lower_por = 1
                    upper_por = 100
                
            for w in keywords:        
                rate = []
                vals = []

                for row in good_cls.values:
                        if t == 'neg':
                            for sent in sent_tokenize(row[0]):
                                if w in self.remove_every(sent.lower()).strip():
                                    rate.append(row[1])
                                    vals.append(sent)
                        else:
                            for sent in sent_tokenize(row[0]):
                                if w in self.remove_every(sent.lower()).strip() and row[1] >= 4:
                                    rate.append(row[1])
                                    vals.append(sent)

                
                if len(vals) > lower_por and len(vals) < upper_por:
                    vals = vals[:2]
                    rate = rate[:2] 

                    n, a = w.split()
                    w = self.morph.parse(n)[0].normal_form + " " + a

                    if (t == 'neg' and np.array(rate).mean() > 3) or (t == 'pos' and np.array(rate).mean() <=3):
                        if w in garbage.keys():
                                garbage[w]['examples'] += vals
                                garbage[w]['rates'] += rate

                        else:
                            garbage[w] = {
                                'examples':vals,
                                'rates':rate,
                            }

                    else:    
                        if w in end_data.keys():
                                end_data[w]['examples'] += vals
                                end_data[w]['rates'] += rate

                        else:
                            end_data[w] = {
                                'examples':vals,
                                'rates':rate,
                            }


            for kwd in end_data.keys():
                end_data[kwd]['mean_rate'] = np.mean(end_data[kwd]['rates'])


            return end_data, garbage
        
        bad_report, garb_neg = prepare_category('neg', self.end_neg, self.data)
        good_report, garb_pos = prepare_category('pos', self.end_pos, self.data)

        end_report = {
            'good_points':{**good_report, **garb_neg},
            'bad_points':{**bad_report, **garb_pos}
        }
        
        return end_report
        
    def classification(self) -> None:
        """
            –§—É–Ω–∫—Ü–∏—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –±–∏–≥—Ä–∞–º
        """
        
        sent_filtered = pd.DataFrame({'text':self.pos_clf_prepared})
        sent_filtered['emb'] = sent_filtered['text'].apply(lambda x: self.extractor.get_text_embedding(text=x, model=self.emb_model, vector_size=300))

        sent_filtered['pred'] = self.clf.predict(np.stack(sent_filtered['emb']))
        pos_sent_filtered = list(sent_filtered[sent_filtered['pred'] == 1]['text'].values)
        pos_sent_filtered = list(set(self.pos_sent_filtered_1) & set(self.pos_sent_normal))
        
        sent_filtered = pd.DataFrame({'text':self.neg_clf_prepared})
        sent_filtered['emb'] = sent_filtered['text'].apply(lambda x: self.extractor.get_text_embedding(text=x, model=self.emb_model, vector_size=300))

        sent_filtered['pred'] = self.clf.predict(np.stack(sent_filtered['emb']))
        neg_sent_filtered = list(sent_filtered[sent_filtered['pred'] == 1]['text'].values)
        neg_sent_filtered = list(set(self.neg_sent_filtered_1) & set(self.neg_sent_normal))
        
        self.end_pos = pos_sent_filtered
        self.end_neg = neg_sent_filtered
        
        return None
        
    def bigrams_work(self) -> None:
        """
            –§—É–Ω–∫—Ü–∏—è —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è –Ω–∞ –±–∏–≥—Ä–∞–º–º—ã –∏ –∏—Ö –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è
        """
        
        def get_bigrams(text):
            """
                –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –≤—Å–µ–≤–æ–∑–º–æ–∂–Ω—ã—Ö –±–∏–≥—Ä–∞–º –≤ —Ñ–æ—Ä–º–∞—Ç–µ 
                "–ø—Ä–∏–ª–∞–≥–∞—Ç–µ–ª—å–Ω–æ–µ" + "—Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ–µ"
                @text - —Ç–µ–∫—Å—Ç –æ—Ç–∑—ã–≤–∞
            """
            
            words = word_tokenize(text)

            nouns = []
            adjs = []
            bigrams = []

            for word in words:
                tag = str(self.morph.parse(word)[0].tag).split(',')[0]
                if tag in ['ADJF','ADJS','PRTF']:
                    adjs.append(word)
                elif tag == 'NOUN':
                    nouns.append(word)

            for noun in nouns:
                for adj in adjs:
                    bigrams.append(noun + ' ' + adj)

            return bigrams
        
        def get_normal_bigrams(text):
            """
                –§—É–Ω–∫—Ü–∏—è –ø–æ–ª—É—á–µ–Ω–∏—è —Ç–æ–ª—å–∫–æ –°–£–©–ï–°–¢–í–£–Æ–©–ò–• –≤ –¥–∞—Ç–∞—Å–µ—Ç–µ –±–∏–≥—Ä–∞–º
                @text - —Ç–µ–∫—Å—Ç –æ—Ç–∑—ã–≤–∞
            """
            
            words = word_tokenize(text)

            bigrams = []

            for i in range(len(words) - 1):
                tag1, tag2 = str(self.morph.parse(words[i])[0].tag).split(',')[0], str(self.morph.parse(words[i + 1])[0].tag).split(',')[0]

                if tag2 in ['ADJF','ADJS','PRTF'] and tag1 == 'NOUN':
                    bigrams.append(words[i] + ' ' + words[i + 1])

            return bigrams
        
        def prep_2_gram(text):
            """
                –§—É–Ω–∫—Ü–∏—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –±–∏–≥—Ä–∞–º–º –ø–æ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—é –æ–¥–Ω–æ–≥–æ –∏–∑ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –≤ —Å–ø–∏—Å–∫–µ —Å—Ç–æ–ø—Å–ª–æ–≤
                @text - —Ç–µ–∫—Å—Ç –¥–≤—É–≥—Ä–∞–º–º—ã
        
        
                P.S. –§—É–Ω–∫—Ü–∏—è –Ω—É–∂–Ω–∞ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –≤ –º–µ—Ç–æ–¥–µ filter()
            """
            
            for word in text.split(' '):
                    if self.lemmer.lemmatize(word)[0] in self.russian_stopwords:
                        return False
            return True  
        
        
        # –∏—Å–ø–æ–ª—å–∑—É–µ–º –≤—Å–µ–≤–æ–∑–º–æ–∂–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –Ω–∏–∂–µ         
        pos_sent = sum([[i for i in get_bigrams(text)] for text in self.pos], [])
        neg_sent = sum([[i for i in get_bigrams(text)] for text in self.neg], [])

        self.pos_sent_normal = sum([[i for i in get_normal_bigrams(text)] for text in self.pos], [])
        self.neg_sent_normal = sum([[i for i in get_normal_bigrams(text)] for text in self.neg], [])

        self.pos_sent_filtered_1 = list(filter(prep_2_gram, pos_sent))
        self.neg_sent_filtered_1 = list(filter(prep_2_gram, neg_sent))
        
        # –ø—Ä–æ–∏–∑–≤–µ–¥–µ–º –∞–Ω–∞–Ω–ª–∏–∑ —á–∞—Å—Ç–æ—Ç–Ω–æ—Å—Ç–∏
        pos_c = dict(Counter(self.pos_sent_filtered_1))
        neg_c = dict(Counter(self.neg_sent_filtered_1))
        
        # –æ—Ç—Ñ–∏–ª—å—Ç—Ä—É–µ–º –ø–æ –≥–ª–æ–±–∞–ª—å–Ω–æ–º—É –ø–æ—Ä–æ–≥–æ–≤–æ–º—É –∑–Ω–∞—á–µ–Ω–∏—é
        pos_dict = pd.DataFrame(dict(filter(lambda x: True if x[1] > self.global_por_pos else False, pos_c.items())).items(), columns=['text', 'count'])
        neg_dict = pd.DataFrame(dict(filter(lambda x: True if x[1] > self.global_por_neg else False, neg_c.items())).items(), columns=['text', 'count'])
        
        # –ø–æ–¥–≥–æ—Ç–æ–≤–∏–º –∫ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
        self.pos_clf_prepared = list(pos_dict.sort_values(by='count', ascending=False)['text'])
        self.neg_clf_prepared = list(neg_dict.sort_values(by='count', ascending=False)['text'])
        
        return None
        
    def data_prep(self) -> None:
        """
            –§—É–Ω–∫—Ü–∏—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å—ã—Ä—ã—Ö –∏ –æ—Ç—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        """
        
        self.raw_data.set_axis(['review', 'rate', 'created_at'], axis='columns', inplace=True)
        self.raw_data['review'] = self.raw_data['review'].apply(lambda x: x.strip().replace('\n','')) 

        # –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å–∏ –æ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –Ω—É–∂–Ω–æ –≤—ã—Å—Ç–∞–≤–∏—Ç—å –ø–æ—Ä–æ–≥–æ–≤—ã–µ –¥–ª—è –æ—Ç–±–æ—Ä–∞ –∑–Ω–∞—á–µ–Ω–∏—è
        if self.raw_data.shape[0] < 1000:
            self.global_por_pos = 0
            self.global_por_neg = 0

            self.data = self.raw_data.drop(['created_at'], axis='columns')
        else:
            self.global_por_pos = 2
            self.global_por_neg = 0

            extracted_data = self.extractor.run(pd.DataFrame({'review':self.raw_data['review']}))

            self.data = pd.DataFrame({'review':extracted_data})
        
        self.data['rate'] = self.data['review'].apply(self.get_star)
        self.data['prepared_review'] = self.data['review'].apply(self.remove_every)
        
        # —Ä–∞–∑–¥–µ–ª–∏–º –Ω–∞ –ø–æ–∑–∏—Ç–∏–≤–Ω—ã–µ –∏ –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã–µ –ø–æ –æ—Ü–µ–Ω–∫–µ –æ—Ç–∑—ã–≤—ã
        self.pos = sum([sent_tokenize(text) for text in list(self.data[self.data['rate'] >= 3]['prepared_review'].values)],[])
        self.neg = sum([sent_tokenize(text) for text in list(self.data[self.data['rate'] < 3]['prepared_review'].values)],[])
        
        return None
        
    def remove_every(self,text):
        """
            –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è, –ø–æ–º–æ–≥–∞—é—â–∞—è –ø–æ—á–∏—Å—Ç–∏—Ç—å –∏—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç –æ—Ç –º—É—Å–æ—Ä–∞,
            –∏—Å–ø–æ–ª—å–∑—É—è —Å—Ç–∞—Ç–∏—á–Ω—ã–µ –º–µ—Ç–æ–¥—ã –∫–ª–∞—Å—Å–∞ RUSentimentExtractor
        """
        
        return self.extractor.remove_garbage(text)
    
    def get_star(self, text):
        """
            –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è, –ø–æ–º–æ–≥–∞—é—â–∞—è –Ω–∞–π—Ç–∏ –æ—Ü–µ–Ω–∫—É –æ—Ç–∑—ã–≤–∞ –≤ –∏—Å—Ö–æ–¥–Ω–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ
            @text - —Ç–µ–∫—Å—Ç –æ—Ç–∑—ã–≤–∞
        """

        try:
            review_star = self.raw_data[self.raw_data['review'] == text.strip()]['rate'].values[0]
            return review_star
        except:
            return None
        