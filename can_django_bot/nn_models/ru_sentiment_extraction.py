import pandas as pd
import numpy as np

import re
import emoji

from sklearn.cluster import DBSCAN
from sklearn.cluster import KMeans

from toolz import pipe

class RUSentimentExtractor:
    """
        –ö–ª–∞—Å—Å, —Å –ø–æ–º–æ—â—å—é –∫–æ—Ç–æ—Ä–æ–≥–æ —Ä–µ–∞–ª–∏–∑—É–µ—Ç—Å—è –≤–µ—Å—å –∞–ª–≥–æ—Ä–∏—Ç–º –≤—ã—á–ª–µ–Ω–µ–Ω–∏—è –ø–æ–ª–µ–∑–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤ –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞
    """
    
    def __init__(self, vectorizer, classifier, cls_eps:float=0.2, cls_metric:str='cosine', cls_min_samples:int=1, vector_size:int=300) -> None:
        self.vectorizer = vectorizer
        self.classifier = classifier
        self.cls_eps = cls_eps
        self.cls_metric = cls_metric
        self.cls_min_samples = cls_min_samples
        self.vector_size = vector_size
        
    def run(self, data:pd.DataFrame) -> np.array:
        """
            –§—É–Ω–∫—Ü–∏—è, –∑–∞–ø—É—Å–∫–∞—é—â–∞—è –∞–ª–≥–æ—Ä–∏—Ç–º
        """
        
        # –∫–∞—Å–∫–∞–¥–Ω–æ –≤—ã–∑–æ–≤–µ–º –≤—Å–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –º–µ—Ç–æ–¥—ã
        useful_texts = pipe(data, self.data_prep, self.clusterization, self.cluster_classification, self.text_classification)
        
        return useful_texts
        
    def data_prep(self, df: pd.DataFrame) -> pd.DataFrame:
        """
            –§—É–Ω–∫—Ü–∏—è, –∫–æ—Ç–æ—Ä–∞—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ
        """
        
        try:
            # –ø—Ä–∏ –≤—ã–≥—Ä—É–∑–∫–µ –≤ —Ñ–∞–π–ª, –±—ã–≤–∞–µ—Ç, —á—Ç–æ –∑–∞–±—ã–≤–∞—é—Ç –∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞—Ç—å –∏–Ω–¥–µ–∫—Å –∏ –æ–Ω –∑–∞–¥–∞–µ—Ç—Å—è –∫–∞–∫ –¥–∞–Ω–Ω–∞—è –∫–æ–ª–æ–Ω–∫–∞ 
            df.drop(['Unnamed: 0'], inplace=True, axis=1)
        except KeyError:
            pass
        
        #—É–¥–∞–ª—è–µ–º –≤—Å–µ nan
        df.dropna(inplace=True)
        
        # –ø—Ä–æ–≤–µ—Ä–∏–º, —á—Ç–æ–±—ã –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–æ–ª–æ–Ω–æ–∫ –±—ã–ª–æ —Ä–∞–≤–Ω–æ 1
        if len(df.columns) > 1:
            raise Exception('Columns amount is not equals to 1')
        
        # –∏–∑–º–µ–Ω–∏–º –Ω–∞–∑–≤–∞–Ω–∏–µ –∫–æ–ª–æ–Ω–∫–∏ –Ω–∞ –Ω—É–∂–Ω–æ–µ –Ω–∞–º
        df.set_axis(['raw_text'],axis=1, inplace=True) 
        
        # –≤–æ—Å–ø–æ–ª—å–∑—É–µ–º—Å—è —Å—Ç–∞—Ç–∏—á–Ω—ã–º–∏ –º–µ—Ç–æ–¥–∞–º–∏ –¥–ª—è –æ—á–∏—Å—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞
        df['text'] = df['raw_text'].apply(RUSentimentExtractor.remove_garbage)
        
        # –¥–æ–±–∞–≤–∏–º –≤–æ—Å–ø–æ–º–≥–∞—Ç–µ–ª—å–Ω—É—é –∫–æ–ª–æ–Ω–∫—É
        df['text_len'] = df['text'].apply(lambda x: len(x.split()))
        
        # —É–¥–∞–ª—è–µ–º –æ–¥–Ω–æ—Å–ª–æ–≤–Ω—ã–µ –≤—ã–±—Ä–æ—Å—ã
        df = df[df['text_len'] > 1] 
        
        # –∑–∞–ø–∏—à–µ–º –≤ —Ç–∞–±–ª–∏—Ü—É embedding–∏ –æ—á–µ—â–µ–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤
        df['emb'] = df['text'].apply(lambda el: RUSentimentExtractor.get_text_embedding(el, self.vectorizer, self.vector_size))
        
        return df
    
    def clusterization(self, data: pd.DataFrame) -> pd.DataFrame:
        """
            –§—É–Ω–∫—Ü–∏—è, –≤–æ–∑–≤—Ä–∞—â–∞—é—â–∞—è –¥–∞—Ç–∞—Å–µ—Ç —Å –∫–ª–∞—Å—Ç–µ—Ä–∞–º–∏ –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–π –∏—Ö –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
        """
        
        # –Ω–∞–π–¥–µ–º –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
        clusters_amount = np.unique(DBSCAN(eps=self.cls_eps, metric=self.cls_metric, min_samples=self.cls_min_samples).fit_predict(np.stack(data['emb'].values))).shape[0]
     
        print('Cluster amount is', clusters_amount, 'ü•∫')
        
        # –∫–ª–∞—Å—Ç–µ—Ä–∏–∑—É–µ–º —Å –ø–æ–º–æ—â—å—é –º–µ—Ç–æ–¥–∞ k —Å—Ä–µ–¥–Ω–∏—Ö
        kmeans = KMeans(n_clusters=clusters_amount).fit_predict(np.stack(data['emb'].values))
        
        # –ø—Ä–∏—Å–≤–æ–∏–º –∫–∞–∂–¥–æ–º—É –æ—Ç–∑—ã–≤—É –µ–≥–æ –∫–ª–∞—Å—Ç–µ—Ä
        data['cluster'] = kmeans
    
        #–ø–æ–ª—É—á–∏–º —Å—ã—Ä—ã–µ –∫–ª–∞—Å—Ç–µ—Ä—ã (–ø–µ—Ä–≤–æ–Ω–∞—á–∞–ª—å–Ω—ã–π –≤–∏–¥) –∏ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
        raw_clusters = []
        prep_clusters = []

        for i in np.unique(kmeans):
            raw_clust = data[data['cluster'] == i]['raw_text'].apply(lambda x: x.replace('\n','').strip())
            raw_clusters.append(np.unique(np.array(raw_clust)))

            prep_clust = data[data['cluster'] == i]['text'].apply(lambda x: x.strip().lower())
            prep_clusters.append(np.unique(np.array(prep_clust)))
        
        # –ø—Ä–∏–≤–µ–¥–µ–º –∫–ª–∞—Å—Ç–µ—Ä–∞ –∫ —Å—Ç—Ä–æ—á–Ω–æ–º—É –≤–∏–¥—É
        cls_prep = list(map(lambda x:' '.join(x), prep_clusters))
        cls_prep_to_clf = list(map(lambda x:' // '.join(x), prep_clusters))
        cls_raw = list(map(lambda x:' // '.join(x), raw_clusters))
        
        # –∑–∞–ø–∏—à–µ–º –≤ dataframe –∏ –≤–µ—Ä–Ω–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        processed_data = pd.DataFrame({
            'cls_prep': cls_prep,
            'cls_raw': cls_raw,
            'cls_prep_to_clf': cls_prep_to_clf
        })
        
        return processed_data
        
    def cluster_classification(self, processed_data: pd.DataFrame) -> pd.DataFrame:
        """
            –§—É–Ω–∫—Ü–∏—è, —Ä–µ–∞–ª–∏–∑—É—é—â–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—é –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
        """
        
        # –ø–æ–ª—É—á–∏–º embedding –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
        processed_data['cls_prep_emb'] = processed_data['cls_prep'].apply(lambda el: RUSentimentExtractor.get_text_embedding(el, self.vectorizer, self.vector_size))

        # —Å–¥–µ–ª–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
        preds = self.classifier.predict(np.stack(processed_data['cls_prep_emb'].values))
        
        # –∑–∞–ø–∏—à–µ–º –≤ –Ω–∞—à–∏ –¥–∞–Ω–Ω—ã–µ
        processed_data['preds'] = preds
        
        # –ø–æ–ª—É—á–∏–º —Ö–æ—Ä–æ—à–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
        good_cls = processed_data[processed_data['preds'] == 1]
        
        return good_cls
    
    def text_classification(self, good_cls: pd.DataFrame) -> np.array:
        """
            –§—É–Ω–∫—Ü–∏—è, —Ä–µ–∞–ª–∏–∑—É—é—â–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—é –æ—Ç–¥–µ–ª—å–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤,
            –≤—Ö–æ–¥—è—â–∏—Ö –≤ —Ö–æ—Ä–æ—à–∏–µ –∫–ª–∞—Å—Ç–µ—Ä–∞
        """
         
        # —Ñ—É–Ω–∫—Ü–∏—è –ø–µ—Ä–µ–≤–æ–¥–∞ –∫–ª–∞—Å—Ç–µ—Ä–∞ –≤ –æ—Ç–¥–µ–ª—å–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã
        def cls_to_text(series: pd.Series) -> np.array:
            end = np.array([])
            for val in list(series.values):
                end = np.concatenate((end, np.array(val.split(' // '))), axis=None)
            return end    
        
        cls_to_clf = cls_to_text(good_cls['cls_prep_to_clf'])
        cls_to_raw = cls_to_text(good_cls['cls_raw'])
        
        if cls_to_clf.shape[0] > cls_to_raw.shape[0]:
            cls_to_clf = cls_to_clf[:cls_to_raw.shape[0]] 
        
        if cls_to_raw.shape[0] > cls_to_clf.shape[0]:
            cls_to_raw = cls_to_raw[:cls_to_clf.shape[0]] 
        
        # –∑–∞–ø–∏—à–µ–º –≤—Å–µ –≤ dataframe
        useful_texts = pd.DataFrame({
            'useful_text': cls_to_clf,
            'useful_text_raw': cls_to_raw
        })
        
        # –∑–∞–ø–∏—Å—ã–≤–∞–µ–º embedding –∫–∞–∂–¥–æ–≥–æ –ø–æ–ª–µ–∑–Ω–æ–≥–æ –æ—Ç–∑—ã–≤–∞
        useful_texts['emb'] = useful_texts['useful_text'].apply(lambda x: RUSentimentExtractor.get_text_embedding(x, self.vectorizer, self.vector_size))
        
        # –¥–µ–ª–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
        preds = self.classifier.predict(np.stack(useful_texts['emb'].values))
        
        # –∑–∞–ø–∏—Å—ã–≤–∞–µ–º –≤ –æ—Ç–¥–µ–ª—å–Ω—É—é –∫–æ–ª–æ–Ω–∫—É
        useful_texts['preds'] = preds
        
        # –±–µ—Ä–µ–º —Ö–æ—Ä–æ—à–∏–µ —Ç–µ–∫—Å—Ç—ã
        good_texts = useful_texts[useful_texts['preds'] == 1]['useful_text_raw'].values
        
        return good_texts
        
    @staticmethod   
    def get_text_embedding(text: str, model, vector_size:int) -> np.array:
        """
            –§—É–Ω–∫—Ü–∏—è –≤–æ–∑–≤—Ä–∞—â–∞—é—â–∞—è embedding —Ç–µ–∫—Å—Ç–∞
        """
        
        embeddings = []

        text_prepared = [word.lower() for word in text.split()]

        for word in text_prepared:
            try:
                vector = model[word]
                embeddings.append(vector)
            except:
                vector = np.zeros(vector_size)
                embeddings.append(vector)

        return np.array(embeddings).mean(axis=0)    
    
    @staticmethod
    def remove_garbage(text: str) -> str:
        """
            –ú–µ—Ç–æ–¥, —É–¥–∞–ª—è—é—â–∏–π –≤–µ—Å—å –º—É—Å–æ—Ä –∏–∑ —Ç–µ–∫—Å—Ç–∞
        """
        
        allchars = [str for str in text]
        emoji_list = [c for c in allchars if c in emoji.UNICODE_EMOJI]
        clean_text = ' '.join([str for str in text.split() if not any(i in str for i in emoji_list)])

        return re.sub(r'\d', '', re.sub(r'[^\w\s]','',clean_text.strip().lower()))
    
   